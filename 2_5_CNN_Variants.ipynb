{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Download data and source folder (**Do not Modify**)\n"
      ],
      "metadata": {
        "id": "vnFvUsHEH7P_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ExnLFOzGrz8",
        "outputId": "b52b96dd-5ea5-4c00-a9fd-1dce3d82a5b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.8.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CQdgTOUlY-TUoWZyxtVZxRthBhSuhDVi\n",
            "To: /content/source.zip\n",
            "100%|██████████| 11.8M/11.8M [00:00<00:00, 302MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "import gdown\n",
        "import zipfile\n",
        "url = 'https://drive.google.com/uc?id=1CQdgTOUlY-TUoWZyxtVZxRthBhSuhDVi'\n",
        "output = 'source.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "with zipfile.ZipFile(output, \"r\") as zip_ref:\n",
        "    zip_ref.extractall('.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install package for calculating FLOPS (**Do not Modify**)\n"
      ],
      "metadata": {
        "id": "h0SUiITHIODO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pthflops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bYv2QizG2Uf",
        "outputId": "1287b2d5-9323-4760-e555-ae9672292404"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pthflops\n",
            "  Downloading pthflops-0.4.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pthflops) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pthflops) (4.1.1)\n",
            "Installing collected packages: pthflops\n",
            "Successfully installed pthflops-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Necessary Dependencies\n",
        "- You may import other packages if you want.\n"
      ],
      "metadata": {
        "id": "MxutNkHbIVDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import os.path as osp\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pthflops import count_ops\n",
        "\n",
        "# functions that you downloaded from the first code cell.\n",
        "# please use this code for seed reset, dataloaders, and test function  \n",
        "from src.util import reset\n",
        "# if data.py is changed, please submit that also.\n",
        "from src.data import get_dataloader\n",
        "from src.test import test"
      ],
      "metadata": {
        "id": "iO9f0-7-HJT2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define label names, data directory, device name.\n"
      ],
      "metadata": {
        "id": "DwrUU3z-IuJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT MODIFY FROM HERE\n",
        "label_names = ['bug', 'electric', 'fighting', 'fire', 'flying', 'grass', 'ground', 'phychic', 'poison', 'water']\n",
        "data_dir = 'data'\n",
        "## DO NOT MODIFY UNTIL HERE\n",
        "\n",
        "## You may modify device name depending on your workspace spec.\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "## Specify the batch size as you want.\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "5cYoZDUKHxHy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **(TO-DO)** DEFINE YOUR `MyModel`\n",
        "- Please do not change the class name. Let `MyModel` be your classifier class name. \n",
        "- Below is just simple example using ResNet18."
      ],
      "metadata": {
        "id": "zc8zxrr7JBep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######## Define your classification model.  #######\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, dim_output=len(label_names)):\n",
        "        super().__init__()\n",
        "\n",
        "        self.convnet = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # 112 -> 56\n",
        "            \n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # 56 -> 28\n",
        "            \n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # 28 -> 14\n",
        "\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # 14 -> 7\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(4096, dim_output)\n",
        "        )\n",
        "        \n",
        "    def forward(self, img):\n",
        "        B, C, H, W = img.shape\n",
        "        # self.layers = []\n",
        "        # out = self.features(img)\n",
        "        out = self.convnet(img)\n",
        "        out = out.view(-1, 512 * 7 * 7)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "####################################################"
      ],
      "metadata": {
        "id": "UBeaeUcsH0P2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* MobileNet v1"
      ],
      "metadata": {
        "id": "iD0bHf9mOKVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######## Define your classification model.  #######\n",
        "# MobileNet v1 (modified)\n",
        "class Depthwise(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.depthwise = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, 3, stride=stride, padding=1, groups=in_channels, bias=False),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU6(),\n",
        "        )\n",
        "\n",
        "        self.pointwise = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU6()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, width_multiplier=1, num_classes=len(label_names), init_weights=True):\n",
        "        super().__init__()\n",
        "        self.init_weights=init_weights\n",
        "        alpha = width_multiplier\n",
        "\n",
        "        self.conv1 = BasicConv2d(3, int(32*alpha), 3, stride=2, padding=1)\n",
        "        self.conv2 = Depthwise(int(32*alpha), int(64*alpha), stride=1)\n",
        "        # down sample\n",
        "        self.conv3 = nn.Sequential(\n",
        "            Depthwise(int(64*alpha), int(128*alpha), stride=2),\n",
        "            Depthwise(int(128*alpha), int(128*alpha), stride=1)\n",
        "        )\n",
        "        # down sample\n",
        "        self.conv4 = nn.Sequential(\n",
        "            Depthwise(int(128*alpha), int(256*alpha), stride=2),\n",
        "            Depthwise(int(256*alpha), int(256*alpha), stride=1)\n",
        "        )\n",
        "        # down sample\n",
        "        self.conv5 = nn.Sequential(\n",
        "            Depthwise(int(256*alpha), int(512*alpha), stride=2),\n",
        "            Depthwise(int(512*alpha), int(512*alpha), stride=1),\n",
        "            # Depthwise(int(512*alpha), int(512*alpha), stride=1),\n",
        "            # Depthwise(int(512*alpha), int(512*alpha), stride=1),\n",
        "            # Depthwise(int(512*alpha), int(512*alpha), stride=1),\n",
        "            # Depthwise(int(512*alpha), int(512*alpha), stride=1),\n",
        "        )\n",
        "        # down sample\n",
        "        self.conv6 = nn.Sequential(\n",
        "            Depthwise(int(512*alpha), int(1024*alpha), stride=2)\n",
        "        )\n",
        "        # down sample\n",
        "        self.conv7 = nn.Sequential(\n",
        "            Depthwise(int(1024*alpha), int(1024*alpha), stride=2)\n",
        "        )\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.linear = nn.Linear(int(512*alpha), num_classes) # 1024 -> 512 (56*56)\n",
        "\n",
        "        # weights initialization\n",
        "        if self.init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        # x = self.conv6(x)\n",
        "        # x = self.conv7(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "    # weights initialization function\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "####################################################"
      ],
      "metadata": {
        "id": "fNn2ClnY36wL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* EfficientNet"
      ],
      "metadata": {
        "id": "gtZoPPAyN9UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Swish activation function\n",
        "class Swish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.sigmoid(x)\n",
        "\n",
        "# SE Block\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, r=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.squeeze = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.excitation = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels * r),\n",
        "            Swish(),\n",
        "            nn.Linear(in_channels * r, in_channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.excitation(x)\n",
        "        x = x.view(x.size(0), x.size(1), 1, 1)\n",
        "        return x\n",
        "\n",
        "class MBConv(nn.Module):\n",
        "    expand = 6\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, se_scale=4, p=0.5):\n",
        "        super().__init__()\n",
        "        # first MBConv is not using stochastic depth\n",
        "        self.p = torch.tensor(p).float() if (in_channels == out_channels) else torch.tensor(1).float()\n",
        "\n",
        "        self.residual = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels * MBConv.expand, 1, stride=stride, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(in_channels * MBConv.expand, momentum=0.99, eps=1e-3),\n",
        "            Swish(),\n",
        "            nn.Conv2d(in_channels * MBConv.expand, in_channels * MBConv.expand, kernel_size=kernel_size,\n",
        "                      stride=1, padding=kernel_size//2, bias=False, groups=in_channels*MBConv.expand),\n",
        "            nn.BatchNorm2d(in_channels * MBConv.expand, momentum=0.99, eps=1e-3),\n",
        "            Swish()\n",
        "        )\n",
        "\n",
        "        self.se = SEBlock(in_channels * MBConv.expand, se_scale)\n",
        "\n",
        "        self.project = nn.Sequential(\n",
        "            nn.Conv2d(in_channels*MBConv.expand, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n",
        "        )\n",
        "\n",
        "        self.shortcut = (stride == 1) and (in_channels == out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # stochastic depth\n",
        "        if self.training:\n",
        "            if not torch.bernoulli(self.p):\n",
        "                return x\n",
        "\n",
        "        x_shortcut = x\n",
        "        x_residual = self.residual(x)\n",
        "        x_se = self.se(x_residual)\n",
        "\n",
        "        x = x_se * x_residual\n",
        "        x = self.project(x)\n",
        "\n",
        "        if self.shortcut:\n",
        "            x= x_shortcut + x\n",
        "\n",
        "        return x\n",
        "\n",
        "class SepConv(nn.Module):\n",
        "    expand = 1\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, se_scale=4, p=0.5):\n",
        "        super().__init__()\n",
        "        # first SepConv is not using stochastic depth\n",
        "        self.p = torch.tensor(p).float() if (in_channels == out_channels) else torch.tensor(1).float()\n",
        "\n",
        "        self.residual = nn.Sequential(\n",
        "            nn.Conv2d(in_channels * SepConv.expand, in_channels * SepConv.expand, kernel_size=kernel_size,\n",
        "                      stride=1, padding=kernel_size//2, bias=False, groups=in_channels*SepConv.expand),\n",
        "            nn.BatchNorm2d(in_channels * SepConv.expand, momentum=0.99, eps=1e-3),\n",
        "            Swish()\n",
        "        )\n",
        "\n",
        "        self.se = SEBlock(in_channels * SepConv.expand, se_scale)\n",
        "\n",
        "        self.project = nn.Sequential(\n",
        "            nn.Conv2d(in_channels*SepConv.expand, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n",
        "        )\n",
        "\n",
        "        self.shortcut = (stride == 1) and (in_channels == out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # stochastic depth\n",
        "        if self.training:\n",
        "            if not torch.bernoulli(self.p):\n",
        "                return x\n",
        "\n",
        "        x_shortcut = x\n",
        "        x_residual = self.residual(x)\n",
        "        x_se = self.se(x_residual)\n",
        "\n",
        "        x = x_se * x_residual\n",
        "        x = self.project(x)\n",
        "\n",
        "        if self.shortcut:\n",
        "            x= x_shortcut + x\n",
        "\n",
        "        return x\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, num_classes=len(label_names), width_coef=1., depth_coef=1., scale=1., dropout=0.2, se_scale=4, stochastic_depth=False, p=0.5):\n",
        "        super().__init__()\n",
        "        channels = [32, 16, 24, 40, 80, 112, 192, 320, 1280]\n",
        "        repeats = [1, 2, 2, 3, 3, 4, 1]\n",
        "        strides = [1, 2, 2, 2, 1, 2, 1]\n",
        "        kernel_size = [3, 3, 5, 3, 5, 5, 3]\n",
        "        depth = depth_coef\n",
        "        width = width_coef\n",
        "\n",
        "        channels = [int(x*width) for x in channels]\n",
        "        repeats = [int(x*depth) for x in repeats]\n",
        "\n",
        "        # stochastic depth\n",
        "        if stochastic_depth:\n",
        "            self.p = p\n",
        "            self.step = (1 - 0.5) / (sum(repeats) - 1)\n",
        "        else:\n",
        "            self.p = 1\n",
        "            self.step = 0\n",
        "\n",
        "\n",
        "        # efficient net\n",
        "        self.upsample = nn.Upsample(scale_factor=scale, mode='bilinear', align_corners=False)\n",
        "\n",
        "        self.stage1 = nn.Sequential(\n",
        "            nn.Conv2d(3, channels[0],3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(channels[0], momentum=0.99, eps=1e-3)\n",
        "        )\n",
        "\n",
        "        self.stage2 = self._make_Block(SepConv, repeats[0], channels[0], channels[1], kernel_size[0], strides[0], se_scale)\n",
        "\n",
        "        self.stage3 = self._make_Block(MBConv, repeats[1], channels[1], channels[2], kernel_size[1], strides[1], se_scale)\n",
        "\n",
        "        self.stage4 = self._make_Block(MBConv, repeats[2], channels[2], channels[3], kernel_size[2], strides[2], se_scale)\n",
        "\n",
        "        self.stage5 = self._make_Block(MBConv, repeats[3], channels[3], channels[4], kernel_size[3], strides[3], se_scale)\n",
        "\n",
        "        self.stage6 = self._make_Block(MBConv, repeats[4], channels[4], channels[5], kernel_size[4], strides[4], se_scale)\n",
        "\n",
        "        self.stage7 = self._make_Block(MBConv, repeats[5], channels[5], channels[6], kernel_size[5], strides[5], se_scale)\n",
        "\n",
        "        self.stage8 = self._make_Block(MBConv, repeats[6], channels[6], channels[7], kernel_size[6], strides[6], se_scale)\n",
        "\n",
        "        self.stage9 = nn.Sequential(\n",
        "            nn.Conv2d(channels[7], channels[8], 1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(channels[8], momentum=0.99, eps=1e-3),\n",
        "            Swish()\n",
        "        ) \n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.linear = nn.Linear(channels[8], num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upsample(x)\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "        x = self.stage5(x)\n",
        "        x = self.stage6(x)\n",
        "        x = self.stage7(x)\n",
        "        x = self.stage8(x)\n",
        "        x = self.stage9(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def _make_Block(self, block, repeats, in_channels, out_channels, kernel_size, stride, se_scale):\n",
        "        strides = [stride] + [1] * (repeats - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(in_channels, out_channels, kernel_size, stride, se_scale, self.p))\n",
        "            in_channels = out_channels\n",
        "            self.p -= self.step\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def efficientnet_b0(num_classes=len(label_names)):\n",
        "    return MyModel(num_classes=num_classes, width_coef=1.0, depth_coef=1.0, scale=1.0,dropout=0.2, se_scale=4)\n",
        "\n",
        "def efficientnet_b1(num_classes=len(label_names)):\n",
        "    return MyModel(num_classes=num_classes, width_coef=1.0, depth_coef=1.1, scale=240/224, dropout=0.2, se_scale=4)\n",
        "\n",
        "def efficientnet_b2(num_classes=len(label_names)):\n",
        "    return MyModel(num_classes=num_classes, width_coef=1.1, depth_coef=1.2, scale=260/224., dropout=0.3, se_scale=4)\n",
        "\n",
        "def efficientnet_b3(num_classes=len(label_names)):\n",
        "    return MyModel(num_classes=num_classes, width_coef=1.2, depth_coef=1.4, scale=300/224, dropout=0.3, se_scale=4)\n",
        "\n",
        "def efficientnet_b4(num_classes=len(label_names)):\n",
        "    return MyModel(num_classes=num_classes, width_coef=1.4, depth_coef=1.8, scale=380/224, dropout=0.4, se_scale=4)\n",
        "\n",
        "def efficientnet_b5(num_classes=len(label_names)):\n",
        "    return MyModel(num_classes=num_classes, width_coef=1.6, depth_coef=2.2, scale=456/224, dropout=0.4, se_scale=4)\n",
        "\n",
        "def efficientnet_b6(num_classes=len(label_names)):\n",
        "    return MyModel(num_classes=num_classes, width_coef=1.8, depth_coef=2.6, scale=528/224, dropout=0.5, se_scale=4)\n",
        "\n",
        "def efficientnet_b7(num_classes=len(label_names)):\n",
        "    return MyModel(num_classes=num_classes, width_coef=2.0, depth_coef=3.1, scale=600/224, dropout=0.5, se_scale=4)"
      ],
      "metadata": {
        "id": "c2GnZUgi0--e"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* MobileNet v2"
      ],
      "metadata": {
        "id": "9rJdmnTXOETC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MobileNet v2\n",
        "# 첫번째 layer에서 사용될 convolution 함수\n",
        "def conv_bn(inp, oup, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.ReLU6(inplace=True)\n",
        "    )\n",
        "\n",
        "# inverted bottleneck layer 바로 다음에 나오는 convolution에 사용될 함수\n",
        "def conv_1x1_bn(inp, oup):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.ReLU6(inplace=True)\n",
        "    )\n",
        "\n",
        "# channel수를 무조건 8로 나누어 떨어지게 만드는 함수\n",
        "def make_divisible(x, divisible_by=8):\n",
        "    import numpy as np\n",
        "    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "\n",
        "    def __init__(self, inp, oup, stride, expand_ratio):\n",
        "        super().__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1,2]\n",
        "\n",
        "        hidden_dim = int(inp * expand_ratio)  # expansion channel\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup  # skip connection이 가능한지 확인 True or False\n",
        "        '''\n",
        "        self.stride == 1 ----> 연산 전 후의 feature_map size가 같다는 의미\n",
        "        inp == oup ----> 채널수도 동일하게 유지된다는 의미\n",
        "        즉 skip connection 가능\n",
        "        '''\n",
        "        if expand_ratio == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # 확장시킬 필요가 없기 때문에 바로 depth wise conv\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.ReLU6(inplace=True),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup)\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw(확장)\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.ReLU6(inplace=True),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.ReLU6(inplace=True),\n",
        "                # pw-linear(축소)\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)  # skip connection (element wise sum)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "\n",
        "    def __init__(self, n_class=len(label_names), input_size=224, width_mult=1.):\n",
        "        super().__init__()\n",
        "        block = InvertedResidual\n",
        "        input_channel = 32\n",
        "        last_channel = 1280\n",
        "\n",
        "        interverted_residual_setting = [\n",
        "            # t, c, n, s\n",
        "            # t : expand ratio\n",
        "            # c : channel\n",
        "            # n : Number of iterations\n",
        "            # s : stride\n",
        "            [1, 16, 1, 1],\n",
        "            [6, 24, 2, 2],\n",
        "            [6, 32, 3, 2],\n",
        "            [6, 64, 4, 2],\n",
        "            [6, 96, 3, 1],\n",
        "            [6, 160, 3, 2],\n",
        "            [6, 320, 1, 1]\n",
        "        ]\n",
        "\n",
        "        # building first layer\n",
        "        assert input_size % 32 == 0\n",
        "        # input_channel = make_divisible(input_channel * width_mult)\n",
        "        self.last_channel = make_divisible(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
        "        self.features = [conv_bn(3, input_channel, 2)]  # feature들을 담을 리스트에 first layer 추가\n",
        "\n",
        "        # building inverted residual blocks\n",
        "        for t, c, n, s in interverted_residual_setting:\n",
        "            output_channel = make_divisible(c * width_mult) if t > 1 else c\n",
        "            for i in range(n):\n",
        "                if i == 0:\n",
        "                    self.features.append(block(input_channel, output_channel, s, t))\n",
        "                else:\n",
        "                    self.features.append(block(input_channel, output_channel, 1, t))  # 반복되는 부분에서 skip connection 가능\n",
        "                input_channel = output_channel\n",
        "        # building last several layers\n",
        "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))  # (batch, 320, 7, 7) -> (batch, 1280, 7, 7)\n",
        "        # make it nn.Sequential\n",
        "        self.features = nn.Sequential(*self.features)\n",
        "\n",
        "        # Average pooling layer\n",
        "        self.avg = nn.AvgPool2d(7, 7)\n",
        "        # building classifier\n",
        "        self.classifier = nn.Linear(self.last_channel, n_class)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # pdb.set_trace()\n",
        "        x = self.features(x)\n",
        "        x = self.avg(x)\n",
        "        x = x.view(-1, self.last_channel)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    # 초기 weight 설정\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                n = m.weight.size(1)\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()"
      ],
      "metadata": {
        "id": "slJ6X9Kb7_CE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **(Optional)** Make your own loss function.\n",
        "- You may change `criterion` as you want. \n",
        "- But make your custom loss function work without changing below lines, which starts from `model.train()`\n"
      ],
      "metadata": {
        "id": "DFYTeLz1KOGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, sample):\n",
        "    ### You may define your own loss function.###\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    #############################################\n",
        "\n",
        "    ### make your code work without changing below lines ###\n",
        "    model.train()    \n",
        "    \n",
        "    input = sample['img'].float().to(device)\n",
        "    label = sample['label'].long().to(device)\n",
        "\n",
        "    pred = model(input)\n",
        "\n",
        "    loss = criterion(pred, label)\n",
        "\n",
        "    num_correct = torch.sum(torch.argmax(pred, dim=-1)==label)\n",
        "\n",
        "    # wandb.watch(model, criterion, log=\"all\", log_freq=1) # to be deleted\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item(), num_correct.item()\n",
        "    ##########################################################"
      ],
      "metadata": {
        "id": "M4ed9AWoH0yC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **(TO-DO)** DEFINE YOUR `get_optimizer`\n",
        "- Please do not change the function name. Let `get_optimizer` be your classifier class name. \n",
        "- This function should return the optimizer for optimizing model parameters properly\n"
      ],
      "metadata": {
        "id": "K1Szzew1Mbfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######## Define your own function for optimizer.  #######\n",
        "def get_optimizer(model, lr=0.01):\n",
        "    return optim.Adam(model.parameters(), lr=lr)\n",
        "##########################################################"
      ],
      "metadata": {
        "id": "dxBrwyvMMXN9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repeat Training with different 10 random seeds.\n",
        "\n",
        "*   Do not change `max_epoch` and `num_seeds`.\n",
        "*   You  may change lines for `MyModel` and `get_optimizer` part if they require additional inputs.\n"
      ],
      "metadata": {
        "id": "fOgJFjMSLV7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX2SP43ZNrDF",
        "outputId": "9a01ed31-2a0c-4fe5-bc9e-0444fc4c1629"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.13.5)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.11)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.29)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5D-73IXNxjB",
        "outputId": "8ee3a944-d6e0-4a9b-85c3-963e42ff3db5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mray_park\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "7D0PDglCZKH9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ./src/data.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMSWRGbW9klT",
        "outputId": "f7b97dd9-e96a-477e-9a6c-a76b533fa8f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import os\n",
            "import os.path as osp\n",
            "import random\n",
            "import numpy as np\n",
            "\n",
            "from torch.utils.data import Dataset, DataLoader\n",
            "from PIL import Image\n",
            "import torchvision.transforms as T\n",
            "\n",
            "\n",
            "def random_split(data_dir, label_names, div_rate=8):\n",
            "    image_fps = []\n",
            "    label_idx = []\n",
            "        \n",
            "    for l_idx, l_name in enumerate(label_names):\n",
            "        curr_label = l_idx\n",
            "        dir_fp = osp.join(data_dir, l_name)\n",
            "        for img_fp in sorted(os.listdir(dir_fp)):\n",
            "            image_fps.append(osp.join(dir_fp, img_fp))\n",
            "            label_idx.append(curr_label)\n",
            "    \n",
            "    num_total = len(image_fps)\n",
            "    num_test = num_total // div_rate\n",
            "    num_train = num_total - num_test\n",
            "\n",
            "    all_indices = np.arange(len(image_fps))\n",
            "    \n",
            "    random.shuffle(all_indices)\n",
            "    test_indices = all_indices[:num_test]\n",
            "    train_indices = all_indices[num_test:]\n",
            "\n",
            "    train_image_fps = [image_fps[idx] for idx in train_indices]\n",
            "    train_label_idx = [label_idx[idx] for idx in train_indices]\n",
            "\n",
            "    test_image_fps = [image_fps[idx] for idx in test_indices]\n",
            "    test_label_idx = [label_idx[idx] for idx in test_indices]\n",
            "    \n",
            "    return train_image_fps, train_label_idx, test_image_fps, test_label_idx\n",
            "\n",
            "\n",
            "class PokemonDataset(Dataset):\n",
            "    def __init__(self, image_fps, label_idx, is_train):\n",
            "        self.image_fps = image_fps\n",
            "        self.label_idx = label_idx \n",
            "        self.is_train = is_train\n",
            "\n",
            "        if self.is_train:\n",
            "            self.transform = T.Compose([T.AutoAugment(), \n",
            "                                        T.ToTensor()])\n",
            "        else:\n",
            "            self.transform = T.ToTensor()\n",
            "\n",
            "    def __len__(self):\n",
            "        return len(self.image_fps)\n",
            "\n",
            "    def __getitem__(self, idx):\n",
            "        curr_image_fp = self.image_fps[idx]\n",
            "        curr_label_idx = self.label_idx[idx]\n",
            "\n",
            "        sample = dict()\n",
            "        sample['img'] = self.transform(Image.open(curr_image_fp))\n",
            "        sample['label'] = curr_label_idx\n",
            "\n",
            "        return sample\n",
            "\n",
            "def get_dataloader(data_dir, label_names, batch_size):\n",
            "    train_image_fps, train_label_idx, test_image_fps, test_label_idx = random_split(data_dir, label_names)\n",
            "    train_dataset = PokemonDataset(train_image_fps, train_label_idx, True)\n",
            "    test_dataset = PokemonDataset(test_image_fps, test_label_idx, False)\n",
            "\n",
            "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
            "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
            "\n",
            "    return train_loader, test_loader, train_dataset, test_dataset"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data.py (colab 파일을 수정해도 반영되지 않아 직접 올립니다.)\n",
        "import os\n",
        "import os.path as osp\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "def random_split(data_dir, label_names, div_rate=8):\n",
        "    image_fps = []\n",
        "    label_idx = []\n",
        "        \n",
        "    for l_idx, l_name in enumerate(label_names):\n",
        "        curr_label = l_idx\n",
        "        dir_fp = osp.join(data_dir, l_name)\n",
        "        for img_fp in sorted(os.listdir(dir_fp)):\n",
        "            image_fps.append(osp.join(dir_fp, img_fp))\n",
        "            label_idx.append(curr_label)\n",
        "    \n",
        "    num_total = len(image_fps)\n",
        "    num_test = num_total // div_rate\n",
        "    num_train = num_total - num_test\n",
        "\n",
        "    all_indices = np.arange(len(image_fps))\n",
        "    \n",
        "    random.shuffle(all_indices)\n",
        "    test_indices = all_indices[:num_test]\n",
        "    train_indices = all_indices[num_test:]\n",
        "\n",
        "    train_image_fps = [image_fps[idx] for idx in train_indices]\n",
        "    train_label_idx = [label_idx[idx] for idx in train_indices]\n",
        "\n",
        "    test_image_fps = [image_fps[idx] for idx in test_indices]\n",
        "    test_label_idx = [label_idx[idx] for idx in test_indices]\n",
        "    \n",
        "    return train_image_fps, train_label_idx, test_image_fps, test_label_idx\n",
        "\n",
        "\n",
        "class PokemonDataset(Dataset):\n",
        "    def __init__(self, image_fps, label_idx, is_train):\n",
        "        self.image_fps = image_fps\n",
        "        self.label_idx = label_idx \n",
        "        self.is_train = is_train\n",
        "\n",
        "        if self.is_train:\n",
        "            self.transform = T.Compose([T.AutoAugment(),\n",
        "                                        T.Resize((56, 56)), \n",
        "                                        T.ToTensor()])\n",
        "        else:\n",
        "            self.transform = T.Compose([T.Resize((56, 56)),\n",
        "                                        T.ToTensor()])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_fps)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        curr_image_fp = self.image_fps[idx]\n",
        "        curr_label_idx = self.label_idx[idx]\n",
        "\n",
        "        sample = dict()\n",
        "        sample['img'] = self.transform(Image.open(curr_image_fp))\n",
        "        sample['label'] = curr_label_idx\n",
        "\n",
        "        return sample\n",
        "\n",
        "def get_dataloader(data_dir, label_names, batch_size):\n",
        "    train_image_fps, train_label_idx, test_image_fps, test_label_idx = random_split(data_dir, label_names)\n",
        "    train_dataset = PokemonDataset(train_image_fps, train_label_idx, True)\n",
        "    test_dataset = PokemonDataset(test_image_fps, test_label_idx, False)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader, train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "10Xy8wL_-1Hd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not change below parameters ###\n",
        "max_epoch = 5\n",
        "num_seeds = 10\n",
        "### Do not change above parameters ###\n",
        "\n",
        "\n",
        "### Make your code work without changing below lines except for \"optimizer\" part ###\n",
        "# wandb.init(project=\"AIToolkit_Assignment04\", entity=\"ray_park\") # to be deleted\n",
        "print('batch size: ', batch_size) # to be deleted\n",
        "total_best_accu = []\n",
        "\n",
        "for seed in range(num_seeds):\n",
        "    # do not erase this random seed initialization part\n",
        "    reset(seed)\n",
        "\n",
        "    # get dataloader by spliting train/test data randomly.\n",
        "    train_loader, test_loader, train_dataset, test_dataset = get_dataloader(data_dir, label_names, batch_size)\n",
        "    print('img size: ', next(iter(train_loader))['img'].shape) # to be deleted\n",
        "\n",
        "    # you may change below line for model definition if your \"MyModel\" requires more inputs.\n",
        "    model = MyModel().to(device)\n",
        "\n",
        "    # you may change below line for optimizer if your \"get optimizer\" requires more inputs.\n",
        "    optimizer = get_optimizer(model)\n",
        "\n",
        "    ###### do not change below lines. Make your code work with below lines. ####\n",
        "    best_accu = -100\n",
        "    for epoch in range(max_epoch):\n",
        "        avg_tr_loss = 0.0\n",
        "        avg_tr_correct = 0.0\n",
        "        for sample in train_loader:\n",
        "            tr_loss, tr_correct = train(model, optimizer, sample)\n",
        "\n",
        "            avg_tr_loss += tr_loss / len(train_loader)\n",
        "            avg_tr_correct += tr_correct / len(train_dataset)\n",
        "\n",
        "        avg_te_correct = 0.0\n",
        "        for sample in test_loader:\n",
        "            te_correct = test(model, sample, device)\n",
        "            avg_te_correct += te_correct / len(test_dataset)\n",
        "    \n",
        "        best_accu = max(avg_te_correct, best_accu)\n",
        "        print('train acc: ', avg_tr_correct) # to be deleted\n",
        "\n",
        "        # wandb.log({\"Epoch\": epoch, \"avg_tr_correct\": avg_tr_correct, \"avg_te_correct\": avg_te_correct}) # to be deleted\n",
        "\n",
        "    print('<<<<<[SEED {}] BEST ACCU : {}>>>>>'.format(seed, best_accu))    \n",
        "    total_best_accu.append(best_accu)\n",
        "    if seed < num_seeds-1:\n",
        "        del model, optimizer\n",
        "    ############################################################################   \n",
        "# wandb.finish() # to be deleted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbowrzBxH06d",
        "outputId": "a97a9487-0de6-4dde-e2ef-f98589a10bc3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch size:  32\n",
            "img size:  torch.Size([32, 3, 56, 56])\n",
            "train acc:  0.17653131452167947\n",
            "train acc:  0.25120440467997274\n",
            "train acc:  0.27735719201651776\n",
            "train acc:  0.3303509979353064\n",
            "train acc:  0.35684790089470075\n",
            "<<<<<[SEED 0] BEST ACCU : 0.37590361445783127>>>>>\n",
            "img size:  torch.Size([32, 3, 56, 56])\n",
            "train acc:  0.19442532690984193\n",
            "train acc:  0.23916035788024803\n",
            "train acc:  0.2828630419821062\n",
            "train acc:  0.31211286992429477\n",
            "train acc:  0.3303509979353063\n",
            "<<<<<[SEED 1] BEST ACCU : 0.39036144578313253>>>>>\n",
            "img size:  torch.Size([32, 3, 56, 56])\n",
            "train acc:  0.18960770818995207\n",
            "train acc:  0.2556779077770134\n",
            "train acc:  0.30591878871300776\n",
            "train acc:  0.31280110116999327\n",
            "train acc:  0.3640743289745356\n",
            "<<<<<[SEED 2] BEST ACCU : 0.472289156626506>>>>>\n",
            "img size:  torch.Size([32, 3, 56, 56])\n",
            "train acc:  0.20991052993805945\n",
            "train acc:  0.2680660701995874\n",
            "train acc:  0.31211286992429466\n",
            "train acc:  0.3317274604267035\n",
            "train acc:  0.3589125946317964\n",
            "<<<<<[SEED 3] BEST ACCU : 0.4409638554216868>>>>>\n",
            "img size:  torch.Size([32, 3, 56, 56])\n",
            "train acc:  0.18685478320715782\n",
            "train acc:  0.2515485203028221\n",
            "train acc:  0.28836889194769466\n",
            "train acc:  0.3444597384721267\n",
            "train acc:  0.384377150722643\n",
            "<<<<<[SEED 4] BEST ACCU : 0.40963855421686746>>>>>\n",
            "img size:  torch.Size([32, 3, 56, 56])\n",
            "train acc:  0.19614590502408835\n",
            "train acc:  0.2749483826565728\n",
            "train acc:  0.33069511355815573\n",
            "train acc:  0.3516861665519615\n",
            "train acc:  0.39366827253957337\n",
            "<<<<<[SEED 5] BEST ACCU : 0.5325301204819276>>>>>\n",
            "img size:  torch.Size([32, 3, 56, 56])\n",
            "train acc:  0.2143840330351\n",
            "train acc:  0.2642807983482453\n",
            "train acc:  0.30178940123881637\n",
            "train acc:  0.32966276668960776\n",
            "train acc:  0.3575361321403994\n",
            "<<<<<[SEED 6] BEST ACCU : 0.46024096385542174>>>>>\n",
            "img size:  torch.Size([32, 3, 56, 56])\n",
            "train acc:  0.20887818306951164\n",
            "train acc:  0.24535443909153506\n",
            "train acc:  0.3017894012388164\n",
            "train acc:  0.3296627666896078\n",
            "train acc:  0.3754301445285618\n",
            "<<<<<[SEED 7] BEST ACCU : 0.4626506024096386>>>>>\n",
            "img size:  torch.Size([32, 3, 56, 56])\n",
            "train acc:  0.20681348933241597\n",
            "train acc:  0.31383344803854113\n",
            "train acc:  0.33172746042670337\n",
            "train acc:  0.3551273227804544\n",
            "train acc:  0.3785271851342052\n",
            "<<<<<[SEED 8] BEST ACCU : 0.4843373493975904>>>>>\n",
            "img size:  torch.Size([32, 3, 56, 56])\n",
            "train acc:  0.18100481761872006\n",
            "train acc:  0.24810736407432932\n",
            "train acc:  0.2549896765313148\n",
            "train acc:  0.31520991052993813\n",
            "train acc:  0.3365450791465935\n",
            "<<<<<[SEED 9] BEST ACCU : 0.508433734939759>>>>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate your grade **(Do not Modify)**\n",
        "- Grade is calculated based on your `average best accuracy` and `FLOPS`.\n",
        "- Higher accuracy and lower flops make your grade better."
      ],
      "metadata": {
        "id": "fAXeguQhNtLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###### do not change below lines #####\n",
        "num_ops = count_ops(model, torch.rand(1, 3, 56, 56).to(device), verbose=False)\n",
        "\n",
        "mean_accu = np.mean(total_best_accu)\n",
        "accu_thres = 0.75\n",
        "accu_point = min(1, np.exp(-2*(accu_thres-mean_accu)))\n",
        "\n",
        "flops = num_ops[0]\n",
        "flops_thres = 2e8\n",
        "flop_disadvantage = flops_thres / (flops_thres + flops)\n",
        "\n",
        "print('*'*50)\n",
        "print('Mean Accuracy :', mean_accu)\n",
        "print('Accuracy Point:', accu_point)\n",
        "print('*'*50)\n",
        "print('Flops: ', flops)\n",
        "print('Flops Advantage: ', flop_disadvantage)\n",
        "print('*'*50)\n",
        "\n",
        "point = accu_point * flop_disadvantage\n",
        "print('Total Points : ', point)\n",
        "\n",
        "threshold = 0.5\n",
        "max_point = 50\n",
        "\n",
        "if point > threshold:\n",
        "    grade = max_point\n",
        "else:\n",
        "    grade = np.exp(-2*(threshold - point)) * max_point\n",
        "\n",
        "print('YOUR grade is {} point'.format(grade))"
      ],
      "metadata": {
        "id": "rkYU6njOH1Dl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfa0c759-086e-452e-f567-9dd4542e5a03"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input size: (1, 3, 56, 56)\n",
            "19,662,730 FLOPs or approx. 0.02 GFLOPs\n",
            "**************************************************\n",
            "Mean Accuracy : 0.45373493975903606\n",
            "Accuracy Point: 0.5529265426880182\n",
            "**************************************************\n",
            "Flops:  19662730\n",
            "Flops Advantage:  0.910486726628591\n",
            "**************************************************\n",
            "Total Points :  0.5034322779180775\n",
            "YOUR grade is 50 point\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xayfUhGhriSW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}